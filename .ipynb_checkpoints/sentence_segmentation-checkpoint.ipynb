{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.utils.importer import CorpusImporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dw = CorpusImporter('hindi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hindi_text_ltrc']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dw.list_corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 100% 6.34 MiB | 958.00 KiB/s \r"
     ]
    }
   ],
   "source": [
    "dw.import_corpus('hindi_text_ltrc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = \" गए, उनका एक समय में बड़ा नाम था। पूरे देश में तालाब बनते थे बनाने वाले भी पूरे देश में थे। कहीं यह विद्या जाति के विद्यालय | सिखाई जाती थी तो कहीं यह जात से हट कर एक विशेष पांत भी जाती थी। बनाने वाले लोग कहीं एक जगह बसे मिलते थे तो कहीं -घूम कर इस काम को करते थे। I 국 घम गजधर एक सुन्दर शब्द है, तालाब बनाने वालों को आदर के साथ याद करने के लिए। राजस्थान के कुछ भागों में यह शब्द आज भी बाकी है।\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " गए, उनका एक समय में बड़ा नाम था। पूरे देश में तालाब बनते थे बनाने वाले भी पूरे देश में थे। कहीं यह विद्या जाति के विद्यालय | सिखाई जाती थी तो कहीं यह जात से हट कर एक विशेष पांत भी जाती थी। बनाने वाले लोग कहीं एक जगह बसे मिलते थे तो कहीं -घूम कर इस काम को करते थे। I 국 घम गजधर एक सुन्दर शब्द है, तालाब बनाने वालों को आदर के साथ याद करने के लिए। राजस्थान के कुछ भागों में यह शब्द आज भी बाकी है।\n"
     ]
    }
   ],
   "source": [
    "print(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "tokenizer = TokenizeSentence('hindi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_para = tokenizer.tokenize_sentences(para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' गए, उनका एक समय में बड़ा नाम था।', 'पूरे देश में तालाब बनते थे बनाने वाले भी पूरे देश में थे।', 'कहीं यह विद्या जाति के विद्यालय |', 'सिखाई जाती थी तो कहीं यह जात से हट कर एक विशेष पांत भी जाती थी।', 'बनाने वाले लोग कहीं एक जगह बसे मिलते थे तो कहीं -घूम कर इस काम को करते थे।', 'I 국 घम गजधर एक सुन्दर शब्द है, तालाब बनाने वालों को आदर के साथ याद करने के लिए।', 'राजस्थान के कुछ भागों में यह शब्द आज भी बाकी है।']\n"
     ]
    }
   ],
   "source": [
    "print(broken_para)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " गए, उनका एक समय में बड़ा नाम था।\n",
      "\n",
      "पूरे देश में तालाब बनते थे बनाने वाले भी पूरे देश में थे।\n",
      "\n",
      "कहीं यह विद्या जाति के विद्यालय |\n",
      "\n",
      "सिखाई जाती थी तो कहीं यह जात से हट कर एक विशेष पांत भी जाती थी।\n",
      "\n",
      "बनाने वाले लोग कहीं एक जगह बसे मिलते थे तो कहीं -घूम कर इस काम को करते थे।\n",
      "\n",
      "I 국 घम गजधर एक सुन्दर शब्द है, तालाब बनाने वालों को आदर के साथ याद करने के लिए।\n",
      "\n",
      "राजस्थान के कुछ भागों में यह शब्द आज भी बाकी है।\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in broken_para:\n",
    "    print(sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenize.word import WordTokenizer\n",
    "tok = WordTokenizer(language='sanskrit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['गए', ',', 'उनका', 'एक', 'समय', 'में', 'बड़ा', 'नाम', 'था।']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.tokenize(broken_para[0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using Indic library functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the local git repo for Indic NLP Library\n",
    "INDIC_NLP_LIB_HOME=\"/home/hinaya/Downloads/indic_nlp_library\"\n",
    "\n",
    "# The path to the local git repo for Indic NLP Resources\n",
    "INDIC_NLP_RESOURCES=\"/home/hinaya/Downloads/indic_nlp_resources\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'{}\\src'.format(INDIC_NLP_LIB_HOME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indicnlp import common\n",
    "common.set_resources_path(INDIC_NLP_RESOURCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/indicnlp/script/indic_scripts.py:106: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  ALL_PHONETIC_VECTORS= ALL_PHONETIC_DATA.ix[:,PHONETIC_VECTOR_START_OFFSET:].as_matrix()\n",
      "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:822: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  retval = getattr(retval, self.name)._getitem_axis(key, axis=i)\n",
      "/usr/local/lib/python3.6/dist-packages/indicnlp/script/indic_scripts.py:106: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  ALL_PHONETIC_VECTORS= ALL_PHONETIC_DATA.ix[:,PHONETIC_VECTOR_START_OFFSET:].as_matrix()\n",
      "/usr/local/lib/python3.6/dist-packages/indicnlp/script/indic_scripts.py:107: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  TAMIL_PHONETIC_VECTORS=TAMIL_PHONETIC_DATA.ix[:,PHONETIC_VECTOR_START_OFFSET:].as_matrix()\n",
      "/usr/local/lib/python3.6/dist-packages/indicnlp/script/indic_scripts.py:107: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  TAMIL_PHONETIC_VECTORS=TAMIL_PHONETIC_DATA.ix[:,PHONETIC_VECTOR_START_OFFSET:].as_matrix()\n",
      "/usr/local/lib/python3.6/dist-packages/indicnlp/script/english_script.py:102: FutureWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#ix-indexer-is-deprecated\n",
      "  ENGLISH_PHONETIC_VECTORS=ENGLISH_PHONETIC_DATA.ix[:,PHONETIC_VECTOR_START_OFFSET:].as_matrix()\n",
      "/usr/local/lib/python3.6/dist-packages/indicnlp/script/english_script.py:102: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  ENGLISH_PHONETIC_VECTORS=ENGLISH_PHONETIC_DATA.ix[:,PHONETIC_VECTOR_START_OFFSET:].as_matrix()\n"
     ]
    }
   ],
   "source": [
    "from indicnlp import loader\n",
    "loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "क़ क़\n",
      "\n",
      "Before normalization\n",
      "0x958 0x20 0x915 0x93c\n",
      "Length: 4\n",
      "\n",
      "After normalization\n",
      "0x915 0x93c 0x20 0x915 0x93c\n",
      "Length: 5\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.normalize.indic_normalize import IndicNormalizerFactory\n",
    "\n",
    "input_text=\"\\u0958 \\u0915\\u093c\"\n",
    "remove_nuktas=False\n",
    "factory=IndicNormalizerFactory()\n",
    "normalizer=factory.get_normalizer(\"hi\")\n",
    "output_text=normalizer.normalize(input_text)\n",
    "\n",
    "print(input_text)\n",
    "print()\n",
    "\n",
    "print('Before normalization')\n",
    "print(' '.join([ hex(ord(c)) for c in input_text ] ))\n",
    "print('Length: {}'.format(len(input_text)))\n",
    "print()    \n",
    "print('After normalization')\n",
    "print(' '.join([ hex(ord(c)) for c in output_text ] ))\n",
    "print('Length: {}'.format(len(output_text)))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "गए, उनका एक समय में बड़ा नाम था।\n",
      "पूरे देश में तालाब बनते थे बनाने वाले भी पूरे देश में थे।\n",
      "कहीं यह विद्या जाति के विद्यालय | सिखाई जाती थी तो कहीं यह जात से हट कर एक विशेष पांत भी जाती थी।\n",
      "बनाने वाले लोग कहीं एक जगह बसे मिलते थे तो कहीं -घूम कर इस काम को करते थे।\n",
      "I 국 घम गजधर एक सुन्दर शब्द है, तालाब बनाने वालों को आदर के साथ याद करने के लिए।\n",
      "राजस्थान के कुछ भागों में यह शब्द आज भी बाकी है।\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.tokenize import sentence_tokenize\n",
    "\n",
    "indic_string= \"गए, उनका एक समय में बड़ा नाम था। पूरे देश में तालाब बनते थे बनाने वाले भी पूरे देश में थे। कहीं यह विद्या जाति के विद्यालय | सिखाई जाती थी तो कहीं यह जात से हट कर एक विशेष पांत भी जाती थी। बनाने वाले लोग कहीं एक जगह बसे मिलते थे तो कहीं -घूम कर इस काम को करते थे। I 국 घम गजधर एक सुन्दर शब्द है, तालाब बनाने वालों को आदर के साथ याद करने के लिए। राजस्थान के कुछ भागों में यह शब्द आज भी बाकी है।\"\n",
    "sentences=sentence_tokenize.sentence_split(indic_string, lang='hi')\n",
    "for t in sentences:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input String: गए, उनका एक समय में बड़ा नाम था।\n",
      "Tokens: \n",
      "सुनो\n",
      ",\n",
      "कुछ\n",
      "आवाज़\n",
      "आ\n",
      "रही\n",
      "है\n",
      "।\n",
      "फोन\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "from indicnlp.tokenize import indic_tokenize  \n",
    "\n",
    "indic_string='सुनो, कुछ आवाज़ आ रही है। फोन?'\n",
    "\n",
    "print('Input String: {}'.format(sentences[0]))\n",
    "print('Tokens: ')\n",
    "for t in indic_tokenize.trivial_tokenize(indic_string): \n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-d67efac8aa08>, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-d67efac8aa08>\"\u001b[0;36m, line \u001b[0;32m34\u001b[0m\n\u001b[0;31m    print i.encode('utf-8')\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import codecs\n",
    "import re\n",
    "class Tokenizer():\n",
    "\t'''class for tokenizer'''\n",
    "\n",
    "\tdef __init__(self,text=None):\n",
    "\t\tif text is  not None:\n",
    "\t\t\tself.text=text.decode('utf-8')\n",
    "\t\t\tself.clean_text()\n",
    "\t\telse:\n",
    "\t\t\tself.text=None\n",
    "\t\tself.sentences=[]\n",
    "\t\tself.tokens=[]\n",
    "\t\tself.stemmed_word=[]\n",
    "\t\tself.final_list=[]\n",
    "\t\t#self.final_tokens=[]\n",
    "\t\n",
    "\n",
    "\tdef read_from_file(self,filename):\n",
    "\t\tf=codecs.open(filename,encoding='utf-8')\n",
    "\t\tself.text=f.read()\n",
    "\t\tself.clean_text()\n",
    "\n",
    "\n",
    "\n",
    "\tdef generate_sentences(self):\n",
    "\t\t'''generates a list of sentences'''\n",
    "\t\ttext=self.text\n",
    "\t\tself.sentences=text.split(u\"।\")\n",
    "\n",
    "\tdef print_sentences(self,sentences=None):\n",
    "\t\tif sentences:\n",
    "\t\t\tfor i in sentences:\n",
    "\t\t\t\tprint (i.encode('utf-8'))\n",
    "\t\telse:\n",
    "\t\t\tfor i in self.sentences:\n",
    "\t\t\t\tprint (i.encode('utf-8'))\n",
    "\n",
    "\n",
    "\tdef clean_text(self):\n",
    "\t\t'''not working'''\n",
    "\t\ttext=self.text\n",
    "\t\ttext=re.sub(r'(\\d+)',r'',text)\n",
    "\t\ttext=text.replace(u',','')\n",
    "\t\ttext=text.replace(u'\"','')\n",
    "\t\ttext=text.replace(u'(','')\n",
    "\t\ttext=text.replace(u')','')\n",
    "\t\ttext=text.replace(u'\"','')\n",
    "\t\ttext=text.replace(u':','')\n",
    "\t\ttext=text.replace(u\"'\",'')\n",
    "\t\ttext=text.replace(u\"‘‘\",'')\n",
    "\t\ttext=text.replace(u\"’’\",'')\n",
    "\t\ttext=text.replace(u\"''\",'')\n",
    "\t\ttext=text.replace(u\".\",'')\n",
    "\t\tself.text=text\n",
    "\n",
    "\tdef remove_only_space_words(self):\n",
    "\n",
    "\t\ttokens=filter(lambda tok: tok.strip(),self.tokens)\n",
    "\t\tself.tokens=tokens\n",
    "\t\t\n",
    "\tdef hyphenated_tokens(self):\n",
    "\n",
    "\t\tfor each in self.tokens:\n",
    "\t\t\tif '-' in each:\n",
    "\t\t\t\ttok=each.split('-')\n",
    "\t\t\t\tself.tokens.remove(each)\n",
    "\t\t\t\tself.tokens.append(tok[0])\n",
    "\t\t\t\tself.tokens.append(tok[1])\n",
    "\n",
    "\n",
    "\n",
    "\tdef tokenize(self):\n",
    "\t\t'''done'''\n",
    "\t\tif not self.sentences:\n",
    "\t\t\tself.generate_sentences()\n",
    "\n",
    "\t\tsentences_list=self.sentences\n",
    "\t\ttokens=[]\n",
    "\t\tfor each in sentences_list:\n",
    "\t\t\tword_list=each.split(' ')\n",
    "\t\t\ttokens=tokens+word_list\n",
    "\t\tself.tokens=tokens\n",
    "\t\t#remove words containing spaces\n",
    "\t\tself.remove_only_space_words()\n",
    "\t\t#remove hyphenated words\n",
    "\t\tself.hyphenated_tokens()\n",
    "\n",
    "\tdef print_tokens(self,print_list=None):\n",
    "\t\t'''done'''\n",
    "\t\tif print_list is None:\n",
    "\t\t\tfor i in self.tokens:\n",
    "\t\t\t\tprint (i.encode('utf-8'))\n",
    "\t\telse:\n",
    "\t\t\tfor i in print_list:\n",
    "\t\t\t\tprint (i.encode('utf-8'))\n",
    "\n",
    "\n",
    "\tdef tokens_count(self):\n",
    "\t\t'''done'''\n",
    "\t\treturn len(self.tokens)\n",
    "\n",
    "\tdef sentence_count(self):\n",
    "\t\t'''done'''\n",
    "\t\treturn len(self.sentences)\n",
    "\n",
    "\tdef len_text(self):\n",
    "\t\t'''done'''\n",
    "\t\treturn len(self.text)\n",
    "\n",
    "\tdef concordance(self,word):\n",
    "\t\t'''done'''\n",
    "\t\tif not self.sentences:\n",
    "\t\t\tself.generate_sentences()\n",
    "\t\tsentence=self.sentences\n",
    "\t\tconcordance_sent=[]\n",
    "\t\tfor each in sentence:\n",
    "\t\t\teach=each.encode('utf-8')\n",
    "\t\t\tif word in each:\n",
    "\t\t\t\tconcordance_sent.append(each.decode('utf-8'))\n",
    "\t\treturn concordance_sent\n",
    "\n",
    "\tdef generate_freq_dict(self):\n",
    "\t\t'''done'''\n",
    "\t\tfreq={}\n",
    "\t\tif not self.tokens:\n",
    "\t\t\tself.tokenize()\n",
    "\n",
    "\t\ttemp_tokens=self.tokens\n",
    "\t\t#doubt whether set can be used here or not\n",
    "\t\tfor each in self.tokens:\n",
    "\t\t\tfreq[each]=temp_tokens.count(each)\n",
    "\n",
    "\t\treturn freq\n",
    "\n",
    "\tdef print_freq_dict(self,freq):\n",
    "\t\t'''done'''\n",
    "\t\tfor i in freq.keys():\n",
    "\t\t\tprint (i.encode('utf-8'),',',freq[i])\n",
    "\n",
    "\tdef generate_stem_words(self,word):\n",
    "\t\tsuffixes = {\n",
    "    1: [u\"ो\",u\"े\",u\"ू\",u\"ु\",u\"ी\",u\"ि\",u\"ा\"],\n",
    "    2: [u\"कर\",u\"ाओ\",u\"िए\",u\"ाई\",u\"ाए\",u\"ने\",u\"नी\",u\"ना\",u\"ते\",u\"ीं\",u\"ती\",u\"ता\",u\"ाँ\",u\"ां\",u\"ों\",u\"ें\"],\n",
    "    3: [u\"ाकर\",u\"ाइए\",u\"ाईं\",u\"ाया\",u\"ेगी\",u\"ेगा\",u\"ोगी\",u\"ोगे\",u\"ाने\",u\"ाना\",u\"ाते\",u\"ाती\",u\"ाता\",u\"तीं\",u\"ाओं\",u\"ाएं\",u\"ुओं\",u\"ुएं\",u\"ुआं\"],\n",
    "    4: [u\"ाएगी\",u\"ाएगा\",u\"ाओगी\",u\"ाओगे\",u\"एंगी\",u\"ेंगी\",u\"एंगे\",u\"ेंगे\",u\"ूंगी\",u\"ूंगा\",u\"ातीं\",u\"नाओं\",u\"नाएं\",u\"ताओं\",u\"ताएं\",u\"ियाँ\",u\"ियों\",u\"ियां\"],\n",
    "    5: [u\"ाएंगी\",u\"ाएंगे\",u\"ाऊंगी\",u\"ाऊंगा\",u\"ाइयाँ\",u\"ाइयों\",u\"ाइयां\"],\n",
    "}\n",
    "\t\tfor L in 5, 4, 3, 2, 1:\n",
    "\t\t\tif len(word) > L + 1:\n",
    "\t\t\t\tfor suf in suffixes[L]:\n",
    "\t\t\t\t\t#print type(suf),type(word),word,suf\n",
    "\t\t\t\t\tif word.endswith(suf):\n",
    "\t\t\t\t\t\t#print 'h'\n",
    "\t\t\t\t\t\treturn word[:-L]\n",
    "\t\treturn word\n",
    "\n",
    "\tdef generate_stem_dict(self):\n",
    "\t\t'''returns a dictionary of stem words for each token'''\n",
    "\t\t# suffixes = {\n",
    "  #   \t\t\t\t1: [\"ो\", \"े\", \"ू\", \"ु\", \"ी\", \"ि\", \"ा\"],\n",
    "  #   \t\t\t\t2: [\"कर\", \"ाओ\", \"िए\", \"ाई\", \"ाए\", \"ने\", \"नी\", \"ना\", \"ते\", \"ीं\", \"ती\", \"ता\", \"ाँ\", \"ां\", \"ों\", \"ें\"],\n",
    "  #   \t\t\t\t3: [\"ाकर\", \"ाइए\", \"ाईं\", \"ाया\", \"ेगी\", \"ेगा\", \"ोगी\", \"ोगे\", \"ाने\", \"ाना\", \"ाते\", \"ाती\", \"ाता\", \"तीं\", \"ाओं\", \"ाएं\", \"ुओं\", \"ुएं\", \"ुआं\"],\n",
    "  #   \t\t\t\t4: [\"ाएगी\", \"ाएगा\", \"ाओगी\", \"ाओगे\", \"एंगी\", \"ेंगी\", \"एंगे\", \"ेंगे\", \"ूंगी\", \"ूंगा\", \"ातीं\", \"नाओं\", \"नाएं\", \"ताओं\", \"ताएं\", \"ियाँ\", \"ियों\", \"ियां\"],\n",
    "  #   \t\t\t\t5: [\"ाएंगी\", \"ाएंगे\", \"ाऊंगी\", \"ाऊंगा\", \"ाइयाँ\", \"ाइयों\", \"ाइयां\"],\n",
    "\t\t# \t\t\t}\n",
    "\n",
    "\t\tstem_word={}\n",
    "\t\tif not self.tokens:\n",
    "\t\t\tself.tokenize()\n",
    "\t\tfor each_token in self.tokens:\n",
    "\t\t\t#print type(each_token)\n",
    "\t\t\ttemp=self.generate_stem_words(each_token)\n",
    "\t\t\t#print temp\n",
    "\t\t\tstem_word[each_token]=temp\n",
    "\t\t\tself.stemmed_word.append(temp)\n",
    "\t\t\t\n",
    "\t\treturn stem_word\n",
    "\n",
    "\tdef remove_stop_words(self):\n",
    "\t\tf=codecs.open(\"rss.txt\",encoding='utf-8')\n",
    "\t\tif not self.stemmed_word:\n",
    "\t\t\tself.generate_stem_dict()\n",
    "\t\tstopwords=[x.strip() for x in f.readlines()]\n",
    "\t\ttokens=[i for i in self.stemmed_word if unicode(i) not in stopwords]\n",
    "\t\tself.final_tokens=tokens\n",
    "\t\treturn tokens\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "\tt=Tokenizer('''वाशिंगटन: दुनिया के सबसे शक्तिशाली देश के राष्ट्रपति बराक ओबामा ने प्रधानमंत्री नरेंद्र मोदी के संदर्भ में 'टाइम' पत्रिका में लिखा, \"नरेंद्र मोदी ने अपने बाल्यकाल में अपने परिवार की सहायता करने के लिए अपने पिता की चाय बेचने में मदद की थी। आज वह दुनिया के सबसे बड़े लोकतंत्र के नेता हैं और गरीबी से प्रधानमंत्री तक की उनकी जिंदगी की कहानी भारत के उदय की गतिशीलता और क्षमता को परिलक्षित करती है।''')\n",
    "\t#t=Tokenizer()\n",
    "\t#t.read_from_file('sample.txt')\n",
    "\t#print type(t.text)\n",
    "\t#y=clean(t.text)\n",
    "\t#print y\n",
    "\tt.generate_sentences()\n",
    "\t#t.print_sentences()\n",
    "\tt.tokenize()\n",
    "\t#t.print_tokens()\n",
    "\tf=t.generate_freq_dict()\n",
    "\t#t.print_freq_dict(f)\n",
    "\ts=t.concordance('बातों')\n",
    "\t#t.print_sentences(s)\n",
    "\tf=t.generate_stem_dict()\n",
    "\t# for i in f.keys():\n",
    "\t# \tprint i.encode('utf-8'),f[i].encode('utf-8')\n",
    "\tz=t.remove_stop_words()\n",
    "\tt.print_tokens(t.final_tokens)\n",
    "\tprint (t.sentence_count(),t.tokens_count(),t.len_text())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
